{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscraping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NovaMaja/webscraping/blob/master/webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YUN0VyJRHduu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Webscraping with Beautiful Soup\n",
        "\n",
        "In this notebook we will be exploring how to use webscraping with  beautiful soup to obtain data about jobs on indeed.com. This notebook and example materials is developed by [Nova Institute](https://novainstitute.ca) and is released under the [MIT license](https://https://github.com/NovaMaja/webscraping/blob/master/LICENSE). "
      ]
    },
    {
      "metadata": {
        "id": "BDajmO_nJG4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports\n",
        "###requests\n",
        "We will use the **requests** library to get the raw html from a webpage. **requests** makes all http requests simple, and you can use it with GET, POST, PUT, DELETE, HEAD, OPTIONS and there are a lot of useful functions included in the library. See http://docs.python-requests.org/ for more info on the **requests** library.\n",
        "\n",
        "###BeautifulSoup\n",
        "**Beautiful Soup** is a library for parsing web pages. It makes a parsing tree of a webpage based on the html structure. With a parsing tree it is easy to navigate through the contents of the webpage and get the information we are looking for. The **Beautiful Soup** [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) has a great quickstart section to get you started.\n",
        "\n",
        "###Pandas\n",
        "**Pandas** is a library for organizing data into series and dataframes (kind of like tables). **pandas** has a lot of built in functions that makes sorting and manipulating data a breeze. Read the Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/) for mor info."
      ]
    },
    {
      "metadata": {
        "id": "SKZDqZO5gnOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7oUc14DMV8w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get and parse the web page\n",
        "The first thing we will do is to go to indeed.com in our browser (we recommend [chrome](https://www.google.com/chrome/)) and do a search for a job we are interested in. In our case we searched for Data Scientist in Toronto, ON.\n",
        "\n",
        "Once we have a serach we are happy with we need to copy the url from the browser and paste it as an argument to the requests.get() function.\n",
        "\n",
        "After we loaded the webpage into our page variable we will pass it on to BautifulSoup to parse it into a parsing tree for us, using *html.parser*"
      ]
    },
    {
      "metadata": {
        "id": "TjdbKDPCg3Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "page = requests.get('https://www.indeed.ca/jobs?q=data+scientist&l=Toronto%2C+ON')\n",
        "soup = BeautifulSoup(page.text, 'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttQFmfYQNc0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "soup has a function called prettify() that makes the parsing tree more human readable. We will use it to print out the information we gathered."
      ]
    },
    {
      "metadata": {
        "id": "jEw9VpUGhcTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eMMlMPjtNrw8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is way too much information to be useful to us. We need to find a way to filter out exactly what we are looking for. A great way to do that is to take a look at the web page using **inspect** in our browser. If we right click on one of the job listings and choose **inspect** chrome will show the relevant html for that element of the website. after a bit of digging around we can see that there is a common keyword for al the jobs in the result list. they all have class *jobsearch-SerpJobCard*. We can use that to get only the information on the cards."
      ]
    },
    {
      "metadata": {
        "id": "nDWufElYiLsP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "jobcards = soup.find_all(class_ = 'jobsearch-SerpJobCard')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdmtWrTKhhEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(jobcards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qd8T2l-4i6C_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(jobcards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uy-k_RXAPo4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the same method we can single out specific information in each job card. We will grab these details and put it into a Dictionary."
      ]
    },
    {
      "metadata": {
        "id": "1j9ipHMdjI6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "jobDict = []\n",
        "\n",
        "for card in jobcards:\n",
        "  links = card.find_all('a')\n",
        "  jobtitle = None\n",
        "  for link in links:\n",
        "    if(link.get('data-tn-element') == 'jobTitle'):\n",
        "      jobtitle = link.get('title')\n",
        "  print(jobtitle)\n",
        "  company = card.find(class_='company')\n",
        "  companylink = company.find('a')\n",
        "  if companylink:\n",
        "    companyName = companylink.contents[0].strip()\n",
        "  else:\n",
        "    companyName = company.contents[0].strip()\n",
        "  print(companyName)\n",
        "  location = card.find(class_='location').contents[0].strip()\n",
        "  print(location)\n",
        "  jobDict.append([companyName, jobtitle, location])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVzswp5zP9yC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can load the dictionary into a pandas dataframe to display it in an orderly fashion. A Pandas dataframe makes it easy to do further analysis on the data."
      ]
    },
    {
      "metadata": {
        "id": "kiLgMnzgkK58",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "jobTable = pd.DataFrame(jobDict, columns=['Company', 'JobTitle', 'Location'])\n",
        "jobTable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR9HvDpVQVuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use the pandas data frame to find only the jobs in the result list that has the title **Data Scientist**"
      ]
    },
    {
      "metadata": {
        "id": "whqp7XVK0hC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "jobTable.loc[jobTable['JobTitle']=='Data Scientist']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
